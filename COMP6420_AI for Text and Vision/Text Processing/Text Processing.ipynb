{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using data that has been derived from the **BioASQ challenge** (http://www.bioasq.org/), after some data manipulation to make it easier to process for this assignment. The BioASQ challenge organises several \"shared tasks\", including a task on biomedical semantic question answering which we are using here. The data are in the file `bioasq10_labelled.csv`, which is part of the zip file provided. Each row of the file has a question, a sentence text, and a label that indicates whether the sentence text is part of the answer to the question (1) or not (0).\n",
    "\n",
    "The following code uses pandas to store the file `bioasq10_labelled.csv` in a data frame and show the first rows of data. For this code to run, first you need to unzip the file `data.zip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "  inflating: bioasq10b_labelled.csv  \n",
      "  inflating: dev_test.csv            \n",
      "  inflating: test.csv                \n",
      "  inflating: training.csv            \n"
     ]
    }
   ],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder \"data\" to combine files:\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the folder name and files\n",
    "new_folder = 'data'\n",
    "files = ['bioasq10b_labelled.csv', 'dev_test.csv', 'test.csv', 'training.csv']\n",
    "\n",
    "# Create the new folder if it doesn't exist\n",
    "if not os.path.exists(new_folder):\n",
    "    os.makedirs(new_folder)\n",
    "\n",
    "# Move each file into the new folder\n",
    "for file_name in files:\n",
    "    if os.path.exists(file_name):\n",
    "        shutil.move(file_name, new_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>sentid</th>\n",
       "      <th>question</th>\n",
       "      <th>sentence text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Hirschsprung disease (HSCR) is a multifactoria...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>In this study, we review the identification of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>The majority of the identified genes are relat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>The non-Mendelian inheritance of sporadic non-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Coding sequence mutations in e.g.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qid  sentid                                           question  \\\n",
       "0    0       0  Is Hirschsprung disease a mendelian or a multi...   \n",
       "1    0       1  Is Hirschsprung disease a mendelian or a multi...   \n",
       "2    0       2  Is Hirschsprung disease a mendelian or a multi...   \n",
       "3    0       3  Is Hirschsprung disease a mendelian or a multi...   \n",
       "4    0       4  Is Hirschsprung disease a mendelian or a multi...   \n",
       "\n",
       "                                       sentence text  label  \n",
       "0  Hirschsprung disease (HSCR) is a multifactoria...      0  \n",
       "1  In this study, we review the identification of...      1  \n",
       "2  The majority of the identified genes are relat...      1  \n",
       "3  The non-Mendelian inheritance of sporadic non-...      1  \n",
       "4                  Coding sequence mutations in e.g.      0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"data/bioasq10b_labelled.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the CSV file are:\n",
    "\n",
    "* `qid`: an ID for a question. Several rows may have the same question ID, as we can see above.\n",
    "* `sentid`: an ID for a sentence.\n",
    "* `question`: The text of the question. In the above example, the first rows all have the same question: \"Is Hirschsprung disease a mendelian or a multifactorial disorder?\"\n",
    "* `sentence text`: The text of the sentence.\n",
    "* `label`: 1 if the sentence is a part of the answer, 0 if the sentence is not part of the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Statistics of part of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `stats_pos` that returns the normalized frequency of all appeared part of speech in the questions and answers (namely the `sentence text` column), respectively. To find the part of speech, use NLTK's \"Universal\" tag set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag_sents\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_pos(csv_file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Extract unique questions and concatenate them into a single string\n",
    "    questions_text = ' '.join(df['question'].unique())\n",
    "    # Concatenate all answer texts into a single string\n",
    "    answers_text = \" \".join(df['sentence text'])\n",
    "\n",
    "    # Tokenize the concatenated text into sentences and then into words\n",
    "    questions_sents = [word_tokenize(sent) for sent in sent_tokenize(questions_text)]\n",
    "    answers_sents = [word_tokenize(sent) for sent in sent_tokenize(answers_text)]\n",
    "\n",
    "    # Tag POS using NLTK's pos_tag_sents with the universal tag set\n",
    "    questions_tags = [tag for sent in pos_tag_sents(questions_sents, tagset='universal') for _, tag in sent]\n",
    "    answers_tags = [tag for sent in pos_tag_sents(answers_sents, tagset='universal') for _, tag in sent]\n",
    "\n",
    "    # Calculate the total count of PoS tags for questions and answers\n",
    "    total_question_tags = len(questions_tags)\n",
    "    total_answer_tags = len(answers_tags)\n",
    "\n",
    "    # Count the frequency of each PoS tag using Counter\n",
    "    questions_freq = Counter(questions_tags)\n",
    "    answers_freq = Counter(answers_tags)\n",
    "\n",
    "    # Normalize and sort the frequencies for questions\n",
    "    questions_normalized = [(tag, round(count / total_question_tags, 4)) \n",
    "                            for tag, count in sorted(questions_freq.items())]\n",
    "    # Normalize and sort the frequencies for answers\n",
    "    answers_normalized = [(tag, round(count / total_answer_tags, 4)) \n",
    "                          for tag, count in sorted(answers_freq.items())]\n",
    "\n",
    "    return questions_normalized, answers_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('.', 0.1201), ('ADJ', 0.0892), ('ADP', 0.1119), ('ADV', 0.011), ('CONJ', 0.0085), ('DET', 0.085), ('NOUN', 0.3536), ('NUM', 0.0056), ('PRON', 0.0377), ('PRT', 0.0104), ('VERB', 0.1659), ('X', 0.0011)], [('.', 0.1236), ('ADJ', 0.1204), ('ADP', 0.117), ('ADV', 0.0245), ('CONJ', 0.0349), ('DET', 0.0769), ('NOUN', 0.3462), ('NUM', 0.0186), ('PRON', 0.01), ('PRT', 0.0159), ('VERB', 0.1112), ('X', 0.0009)])\n"
     ]
    }
   ],
   "source": [
    "print(stats_pos('data/dev_test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing POS distributions between questions and answers:**<p>\n",
    "**Similarities:**\n",
    "- Both have quite similar distributions for NOUN and '.'\n",
    "- The distribution of DET (determiner) is also quite similar in both cases.\n",
    "\n",
    "**Differences:**\n",
    "- VERB is higher in questions (0.1659) than in answers (0.1112). This aligns with questions typically involving actions or inquiries.\n",
    "- ADJ (adjective) is higher in answers (0.1204) than in questions (0.0892),suggesting that answers may contain more descriptive information.\n",
    "- ADV (adverb) is higher in answers (0.0245) than in questions (0.011), indicating that answers may use adverbs for more context or modification.\n",
    "\n",
    "Overall, while there are similarities in certain tags, the differences highlight the distinct functions of questions and answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistics of the top stem n-grams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `stats_top_stem_ngrams` that returns the N most frequent n-gram of stems together with their normalized frequency for questions and answers, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_top_stem_ngrams(csv_file_path, n, N):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Extract unique questions and concatenate them into a single string\n",
    "    questions_text = ' '.join(df['question'].unique())\n",
    "    # Concatenate all answer texts into a single string\n",
    "    answers_text = \" \".join(df['sentence text'])\n",
    "\n",
    "    # Define a Porter stemmer from the NLTK package for stemming\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    def tokenize_and_stem(text):\n",
    "        # Tokenize the text into sentences using NLTK\n",
    "        sentences = sent_tokenize(text)\n",
    "        stemmed_sentences = []\n",
    "        for sentence in sentences:\n",
    "            # Tokenize each sentence into words\n",
    "            words = word_tokenize(sentence)\n",
    "            # Stem each word and collect the result\n",
    "            stemmed_words = [stemmer.stem(word) for word in words]\n",
    "            stemmed_sentences.append(stemmed_words)\n",
    "        return stemmed_sentences\n",
    "\n",
    "    # Apply tokenization and stemming to both questions and answers\n",
    "    questions_stem = tokenize_and_stem(questions_text)\n",
    "    answers_stem = tokenize_and_stem(answers_text)\n",
    "    \n",
    "    def get_ngrams(stemmed_sentences, n):\n",
    "        all_ngrams = []\n",
    "        for sentence in stemmed_sentences:\n",
    "            # Generate n-grams of specified length 'n' if the sentence is long enough\n",
    "            sentence_ngrams = list(ngrams(sentence, n)) if len(sentence) >= n else []\n",
    "            all_ngrams.extend(sentence_ngrams)\n",
    "        return all_ngrams\n",
    "\n",
    "    # Generate n-grams for questions and answers\n",
    "    questions_ngrams = get_ngrams(questions_stem, n)\n",
    "    answers_ngrams = get_ngrams(answers_stem, n)\n",
    "    \n",
    "    def calculate_normalized_frequencies(ngrams_list):\n",
    "        # Compute frequency distribution of n-grams using NLTK's FreqDist\n",
    "        freq_dist = FreqDist(ngrams_list)\n",
    "        total = sum(freq_dist.values())  # Calculate total occurrences\n",
    "        # Normalize frequencies and sort by frequency in descending order\n",
    "        normalized_frequencies = [(ngram, round(freq / total, 4)) for ngram, freq in freq_dist.items()]\n",
    "        # Return the top N n-grams by frequency\n",
    "        return sorted(normalized_frequencies, key=lambda x: x[1], reverse=True)[:N]\n",
    "\n",
    "    # Calculate normalized frequencies for questions and answers, keeping the top N\n",
    "    questions_freq_stem = calculate_normalized_frequencies(questions_ngrams)\n",
    "    answers_freq_stem = calculate_normalized_frequencies(answers_ngrams)\n",
    "    \n",
    "    return questions_freq_stem, answers_freq_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(('what', 'is'), 0.0294), (('is', 'the'), 0.0265), (('of', 'the'), 0.0104), (('in', 'the'), 0.006), (('are', 'the'), 0.0055)], [(('of', 'the'), 0.0064), (('in', 'the'), 0.0054), ((',', 'and'), 0.0054), ((')', ','), 0.0041), (('is', 'a'), 0.0029)])\n"
     ]
    }
   ],
   "source": [
    "print(stats_top_stem_ngrams('data/dev_test.csv', 2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set n=2, N=5, comparing overlap distributions between questions and answers: <p>\n",
    "**Overlaps:**\n",
    "- Both questions and answers include the bigrams ('of', 'the') and ('in', 'the') among their most frequent. This indicates a shared focus on specific entities or locations.\n",
    "**Differences:**\n",
    "- The bigram (\"what\",\"is\") is common in questions, reflecting an inquiry-based approach.\n",
    "- Answers feature the bigram (',', 'and') and ((')', ',') which indicate a tendency towards more descriptive or explanatory content.\n",
    "\n",
    "Overall, while there's some overlap in common phrases, the differences highlight the distinct purposes of questions and answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistics of Named Entity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `stats_ne` that returns the normalized frequency of all named entity types for questions and answers, respectively. Using the default entity types of spacy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/viviannguyen/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/viviannguyen/anaconda3/envs/python310/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Import libraries \n",
    "import os\n",
    "os.system(\"python -m spacy download en_core_web_sm\")\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_ne(csv_file_path):\n",
    "    # Load the en_core_web_sm \n",
    "    nlp = en_core_web_sm.load()\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Extract unique questions and concatenate them into a single string\n",
    "    questions_text = \" \".join(df['question'].unique())\n",
    "    # Concatenate all answer texts into a single string\n",
    "    answers_text = \" \".join(df['sentence text'])\n",
    "\n",
    "    # Ensure both questions and answers have the same text length\n",
    "    min_length = min(len(questions_text), len(answers_text))\n",
    "    questions_text = questions_text[:min_length]\n",
    "    answers_text = answers_text[:min_length]\n",
    "\n",
    "    # Process the questions text with spaCy to extract named entities\n",
    "    questions_doc = nlp(questions_text)\n",
    "    questions_entities = [ent.label_ for ent in questions_doc.ents]\n",
    "\n",
    "    # Process the answers text with spaCy to extract named entities\n",
    "    answers_doc = nlp(answers_text)\n",
    "    answers_entities = [ent.label_ for ent in answers_doc.ents]\n",
    "\n",
    "    # Count the named entity types for questions and answers\n",
    "    questions_ne_counts = Counter(questions_entities)\n",
    "    answers_ne_counts = Counter(answers_entities)\n",
    "\n",
    "    # Calculate the total counts for normalization\n",
    "    total_questions_ents = sum(questions_ne_counts.values())\n",
    "    total_answers_ents = sum(answers_ne_counts.values())\n",
    "\n",
    "    # Normalize and sort the frequencies for questions\n",
    "    questions_freqs = [(ent, round(count / total_questions_ents, 4)) \n",
    "                       for ent, count in sorted(questions_ne_counts.items())]\n",
    "    # Normalize and sort the frequencies for answers\n",
    "    answers_freqs = [(ent, round(count / total_answers_ents, 4)) \n",
    "                     for ent, count in sorted(answers_ne_counts.items())]\n",
    "\n",
    "    # Return the normalized frequencies as tuples for questions and answers\n",
    "    return questions_freqs, answers_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('CARDINAL', 0.0966), ('DATE', 0.0207), ('EVENT', 0.0046), ('FAC', 0.0046), ('GPE', 0.1172), ('LAW', 0.0092), ('LOC', 0.0138), ('NORP', 0.0529), ('ORDINAL', 0.0115), ('ORG', 0.3977), ('PERCENT', 0.0023), ('PERSON', 0.2115), ('PRODUCT', 0.0483), ('QUANTITY', 0.0023), ('WORK_OF_ART', 0.0069)], [('CARDINAL', 0.1887), ('DATE', 0.013), ('FAC', 0.0043), ('GPE', 0.0564), ('LAW', 0.0174), ('LOC', 0.0043), ('MONEY', 0.0022), ('NORP', 0.0412), ('ORDINAL', 0.026), ('ORG', 0.4512), ('PERCENT', 0.013), ('PERSON', 0.1367), ('PRODUCT', 0.0347), ('QUANTITY', 0.0043), ('TIME', 0.0043), ('WORK_OF_ART', 0.0022)])\n"
     ]
    }
   ],
   "source": [
    "print(stats_ne('data/dev_test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Named entity statistics for questions and answers:** <p>\n",
    "**Commonality:**\n",
    "- Both questions and answers heavily feature 'ORG' (organization) entities, with 'PERSON' also being prominent. This aligns with common topics in biomedical texts, where organizations and individuals play key roles.\n",
    "\n",
    "**Differences:**\n",
    "- 'CARDINAL' entities (numeric values) are much more frequent in answers (0.1887) compared to questions (0.0966), suggesting that answers often contain more precise numeric information.\n",
    "- 'GPE' (geopolitical entities) appear more in questions (0.1172) than in answers (0.0564), indicating that questions may focus more on geographical or political topics.\n",
    "- 'DATE' entities are more common in questions (0.0207) compared to answers (0.013), possibly indicating a focus on specific time frames in questions.\n",
    "- Conversely, 'ORDINAL' entities, indicating positions in a series, are more frequent in answers (0.026) than in questions (0.0115)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistics of tf.idf-based similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/viviannguyen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/viviannguyen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/viviannguyen/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "nltk.download('punkt') # For tokenization \n",
    "nltk.download('averaged_perceptron_tagger') #For POS tagging\n",
    "nltk.download('universal_tagset') # For the Universal POS tags\n",
    "import spacy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `stats_tfidf` that returns the ratio of questions that its most similar sentence falls in its answers. That means you need to calculate the cosine similarity between one question and all sentences in the `sentence text` column, and check whether the sentence with the highest similarity falls in the answers of the question. To compute the tf.idf, use sklearn's TfidfVectorizer with the option to remove the English stop words (stop_words='english'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_tfidf(csv_file_path):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Extracting questions and their corresponding answers\n",
    "    unique_questions = df['question'].unique().tolist()\n",
    "    sentences_df = df[['qid', 'sentid', 'sentence text', 'label']]\n",
    "    sentences = df['sentence text'].tolist()\n",
    "    \n",
    "    # Combine all unique questions and answers \n",
    "    all_texts = unique_questions + sentences\n",
    "\n",
    "    # Vectorize the text using TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "    \n",
    "    # Split the matrix into questions and sentences parts\n",
    "    questions_tfidf = tfidf_matrix[:len(unique_questions)]\n",
    "    sentences_tfidf = tfidf_matrix[len(unique_questions):]\n",
    "    \n",
    "    correct_count = 0\n",
    "    for i, question_tfidf in enumerate(questions_tfidf):\n",
    "        # Calculate cosine similarity between each question and all sentences\n",
    "        cosine_similarities = cosine_similarity(question_tfidf, sentences_tfidf)\n",
    "        # Find the index of the highest similarity sentence for each question\n",
    "        most_similar_index = cosine_similarities.argmax()\n",
    "        # Retrieve the most similar sentence\n",
    "        most_similar_sentence = sentences_df.iloc[most_similar_index]\n",
    "        # Retrieve the question ID for the current question\n",
    "        question_id = df[df['question'] == unique_questions[i]].iloc[0]['qid']\n",
    "        # Check if the most similar sentence is a correct answer\n",
    "        if (most_similar_sentence['qid'] == question_id and\n",
    "            most_similar_sentence['label'] == 1):\n",
    "            correct_count += 1\n",
    "            \n",
    "    # Calculate the ratio of questions where the most similar sentence is correct\n",
    "    ratio = round(correct_count / len(unique_questions), 4)\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4876\n"
     ]
    }
   ],
   "source": [
    "print(stats_tfidf('data/dev_test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The result indicates that for approximately 48.76% of the questions, the sentence with the highest cosine similarity based on tf-idf falls within the corresponding answers. This suggests that nearly half of the questions have at least one sentence that closely aligns with their intended answers.\n",
    "- The outcome highlights the challenge of achieving perfect alignment between questions and their correct answers using tf-idf similarity, potentially due to variations in wording or focus between them.\n",
    "- The use of `TfidfVectorizer` with `stop_words='english'` helped in reducing noise from common words, focusing the similarity calculations on more meaningful terms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
